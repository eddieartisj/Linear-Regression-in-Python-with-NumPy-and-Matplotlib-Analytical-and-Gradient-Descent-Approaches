# Linear-Regression-in-Python-with-NumPy-and-Matplotlib-Analytical-and-Gradient-Descent-Approaches
A complete implementation of simple linear regression using both the analytical (closed-form) method and gradient descent, built as part of Purdue Universityâ€™s Machine Learning Foundations course. Includes gradient descent visualization, learning-rate comparison, and fully annotated Python code.

## ğŸ¯ Purpose

This project was completed as part of the Machine Learning Foundations course in Purdue Universityâ€™s Cybersecurity Workforce Certification Training Program.
The purpose of this lab was to explore the fundamentals of linear regression, implement it using two approaches, and understand how learning rates influence the gradient descent optimization process.

## ğŸ“‹ Topics Covered

Synthetic data generation using NumPy

Scatterplot visualization with Matplotlib

Simple Linear Regression (Analytical Method)

Cost Function (Mean Squared Error)

Gradient Descent Optimization

Learning Rate Comparison

Convergence Visualization

## Requirements

To run this project, you will need:

- Python 3.8+

- NumPy

- Matplotlib

- Jupyter Notebook or Google Colab

## Install required packages:

pip install Numpy, Matplotlib, and Jupyter Notebook


Or simply upload the provided .ipynb file to Google Colab.

## ğŸ“ Project Structure
â”œâ”€â”€  Machine_Learning_Foundations_Lab_3_(Final).ipynb     # Final annotated notebook

â”œâ”€â”€  README.md                        # Project documentation

â””â”€â”€  figures/                         # (Optional) Saved plots and outputs

If you plan to store images or export figures, place them inside the figures/ directory.

## Tasks Completed

### **âœ” Task 1**: Analytical Method of Linear Regression

Generated synthetic linear data

Computed mean values of X and y

Calculated line of best fit using closed-form formulas

Plotted equation on top of the dataset

### **âœ” Task 2**: Gradient Descent Method

Implemented a custom cost function

Built gradient descent algorithm from scratch

Tracked parameter updates (Î¸â‚€ and Î¸â‚)

Visualized cost reduction over time

### **âœ” Task 3**: Learning Rate Comparison

Created a function to illustrate convergence behavior

Evaluated multiple learning rates (0.001, 0.01, 0.05, 0.1)

Compared regression line adjustments and cost curves

Observed underfitting, stable learning, and divergence patterns

## ğŸ“· Visual Outputs

The notebook generates several plots, including:

- Scatter Plot of Synthetic Data

- Best-Fit Line (Analytical Method)

- Cost Function Plot (Gradient Descent)

- Learning Rate Comparison Grid

- Shows regression line progression

- Displays cost curve for each learning rate

All visuals are produced directly when running the notebook cells.

## Screenshots
